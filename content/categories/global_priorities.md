---
id: global-priorities
name: 'Global Priorities Research'
effects:
  - effectId: standard
    startTime: 5
    windowLength: 25
    costPerQALY: 10
---

# Justification of cost per life

_The following analysis was done on November 16th 2025. It was written by GPT 5 Thinking and edited by Impact List staff for brevity._

This category covers **Global Priorities Research (GPR)**: systematic research into **which global problems, interventions, and strategies are most important** for altruistic actors to focus on, and how to compare them.

Canonical examples include:

- Academic and applied work at the **Global Priorities Institute (GPI)** at Oxford, which describes its mission as “foundational research on how most effectively to do good” ([GPI research agenda](https://www.globalprioritiesinstitute.org/wp-content/uploads/GPI-research-agenda-version-2.1.pdf)).
- Cause-prioritisation teams in large foundations, such as **Open Philanthropy’s Cause Prioritization team**, which investigates new cause areas and informs how billions of dollars are allocated ([Open Philanthropy cause selection](https://www.openphilanthropy.org/cause-selection/); [cause prioritization team description](https://www.openphilanthropy.org/careers/research-fellow-and-strategy-fellow-global-health-and-wellbeing-cause-prioritization-team/)).
- Cross-cause prioritisation work at independent think tanks (for example Rethink Priorities’ early cause-prioritisation work).

Global priorities research asks questions like:

- “How do we compare the value of working on global health vs factory farming vs preventing future pandemics?”
- “How seriously should we take longtermism—the idea that the far future might matter enormously?”
- “How should we divide limited funding between near-term and long-term causes?”

Definitions along these lines are given by [80,000 Hours](https://80000hours.org/problem-profiles/global-priorities-research/) and [Giving What We Can](https://www.givingwhatwecan.org/effective-altruism-effective-giving/global-priorities).

We treat GPR as a **normal QALY effect**: it increases the number of QALYs generated by other cause areas per dollar, by improving **where big pots of money and talent go**.

> Important: in this framework, **movement-building and worldview-change work** (e.g. effective altruism outreach, longtermism books aimed at the public) is included in **Meta and Theory**, not here. Global Priorities Research is about **technical cause prioritisation and strategy**, especially for large funders.

---

## Headline estimates

- **Cost per QALY (point estimate):**  
  **\$10 per QALY**

- **Plausible range:**  
  **\$1–\$500 per QALY**

This means that, in expectation, a donation of \$10 to top global priorities research produces about 1 extra QALY of benefit for humans, mainly by improving how other money is spent.

The range is very wide because GPR is **hits-based**: many projects have little or no effect, but some change how **billions of dollars** are allocated.

---

## Why global priorities research can be extremely impactful

There are three key reasons GPR can be extraordinarily leveraged:

1. **The stakes are huge.**  
   80,000 Hours notes that governments and philanthropists spend **tens of trillions of dollars** each year on efforts at least nominally aimed at the common good, and that even if only a tiny fraction is responsive to better research, “if billions of dollars could be redirected to problems that are larger in scale, more neglected, or easier to solve, this could provide huge gains” ([80,000 Hours problem profile](https://80000hours.org/problem-profiles/global-priorities-research/)).

2. **Differences in cost-effectiveness across causes are enormous.**  
   80,000 Hours’ framework for comparing global problems suggests that some causes can be **orders of magnitude** more effective than others, even after adjusting for uncertainty ([problem framework](https://80000hours.org/articles/problem-framework/)). Our own cause area estimates show this: for example, AI existential risk work at **\$1.59 per QALY**, animal welfare at **\$8 per QALY**, global health at **\$90 per QALY**, and science & tech at **\$60,000 per QALY**. Choosing different mixtures of these causes makes a gigantic difference.

3. **Past GPR has already led to major shifts.**  
   Global priorities research has helped the effective altruism community identify AI existential risk, biosecurity/pandemics, nuclear risk, and animal welfare as **top-priority areas**. Giving What We Can explicitly notes that GPR has “helped to identify the importance of safeguarding the long-term future, and bolstered the case for devoting greater resources to biosecurity and beneficial artificial intelligence” ([GWWC global priorities page](https://www.givingwhatwecan.org/effective-altruism-effective-giving/global-priorities)).

   Open Philanthropy’s cause-selection work, grounded in cause-prioritisation research, has led it to commit billions of dollars across areas like AI safety, global health, criminal justice reform, and biosecurity with a deliberate “hits-based giving” approach ([Open Phil history and cause selection](https://www.openphilanthropy.org/cause-selection/); [Open Phil notable lessons](https://www.openphilanthropy.org/notable-lessons/)).

The idea is simple: if research can move even **1–5%** of a large funder’s budget from “pretty good” causes to “truly outstanding” ones, it can be **more cost-effective than donating directly** to those causes.

---

## BOTEC 1 – Improving the portfolio of a large foundation

Consider a stylised example:

- Suppose that over the next 20 years, a cluster of large, impact-oriented foundations and donors will allocate a total of **\$10 billion** across global health, poverty, animal welfare, existential risk, and similar areas.
- Without additional GPR, their average cost-effectiveness might be something like **\$200 per QALY** across this mix (roughly between our global health figures at \$90/QALY and more speculative areas with higher or lower cost-effectiveness).
- A GPR programme costing **\$20 million** could, through reports, frameworks, and analyses, modestly improve their decisions:
  - Perhaps shifting 5–10% of their budget into higher-leverage cause areas, or
  - Improving choices _within_ each cause (for example, better picking which global health or pandemic interventions to fund).

Suppose that, in expectation, this **improves the average cost-effectiveness of the \$10 billion portfolio by 2%**. Then:

- Extra “effective money” at the old bar:  
  \$10 billion × 2% = **\$200 million**.
- At a baseline of **\$200 per QALY**, this yields:

  $$
  Q_{\text{extra}} \approx \frac{\$200\ \text{million}}{\$200/\text{QALY}} = 1{,}000{,}000\ \text{QALYs}.
  $$

- The GPR cost is \$20 million, so the **cost per QALY** for GPR is:

  $$
  \text{cost per QALY} \approx \frac{\$20\ \text{million}}{1{,}000{,}000} = \$20.
  $$

This is with only a **2% improvement** on a **\$10 billion** portfolio. If GPR instead improves a larger portfolio, or improves the portfolio by more than 2%, the cost per QALY can easily drop to **single digits or less**.

For example:

- If the same \$20 million of GPR influences **\$20 billion** of spending by 2%, the cost per QALY falls to **\$10**.
- If it influences **\$10 billion** but improves it by only 1%, the cost per QALY rises to **\$40**.

These numbers are intentionally conservative: they assume no influence on government spending and ignore improvements to future generations’ wellbeing.

---

## BOTEC 2 – Discovering or validating new top-priority causes

Global priorities research can also have **“hit-based” impacts** by identifying _new_ cause areas or strongly shifting estimates of existing ones. Historical examples within the effective altruism community include:

- Recognising **AI existential risk** as a top priority, leading to substantial funding for AI safety work ([80,000 Hours on GPR](https://80000hours.org/problem-profiles/global-priorities-research/); [GWWC global priorities](https://www.givingwhatwecan.org/effective-altruism-effective-giving/global-priorities)).
- Elevating **biosecurity and pandemic preparedness** as major global catastrophic risks.
- Emphasising **animal welfare**, especially factory farming, as a moral catastrophe.
- Highlighting **nuclear war** and other global catastrophic risks as large-scale but neglected.

To illustrate how powerful such a “hit” can be in QALY terms, consider:

- A research programme costing **\$10 million** that, over time, persuades a large foundation to allocate **\$500 million** toward AI existential risk work that would otherwise have gone to generic global health efforts.

Using our existing cause-area figures:

- **Without** the GPR hit:

  - \$500 million allocated to global health at **\$90 per QALY**  
    → about **5.6 million QALYs**.

- **With** the GPR hit:
  - \$500 million allocated to AI existential risk work at **\$1.59 per QALY**  
    → about **314 million QALYs**, counting only benefits to currently existing and near-future people.

The **difference** is roughly **308 million QALYs**. Even if:

- Only **10%** of that shift is truly attributable to this specific research programme, and
- We discount the rest as due to other factors,

we still get:

- **30.8 million extra QALYs** from the research, at a cost of \$10 million.
- That’s about **3 QALYs per dollar**, or a **cost per QALY of roughly \$0.30**.

This toy example is deliberately extreme and not representative of the average GPR project. But it shows why GPR is considered **one of the most leveraged fields** by 80,000 Hours and others ([80,000 Hours career review on global priorities research](https://80000hours.org/career-reviews/global-priorities-researcher/)).

In reality:

- Only a **small fraction** of GPR projects will have “hit-level” impact.
- Many will have little or no observable effect on funding decisions.
- Some may be wrong or misleading, pushing resources in less effective directions.

The expected value is therefore a **mix** of rare, huge hits and many low-impact projects.

---

## Putting it together: a conservative central estimate

The two BOTECs above suggest something like:

- In **modest, incremental improvement mode**, GPR may achieve around **\$20–\$50 per QALY**, by slightly improving large philanthropic portfolios.
- In **occasional hit mode**, GPR can be far better, with “blockbuster” projects that effectively buy QALYs at **well under \$1** by redirecting hundreds of millions of dollars towards vastly more effective causes.

Given that:

- We are aiming for a **single, conservative average** across all top GPR charities,
- We only count QALYs for people alive in the next several decades (ignoring the potentially enormous benefits to future generations), and
- We want to allow for many GPR projects having little impact,

we take:

- **Point estimate:** **\$10 per QALY**
- **Plausible range:** **\$1–\$500 per QALY**

The **low end** (\$1–\$5 per QALY) reflects scenarios where:

- GPR meaningfully influences multi-billion-dollar portfolios,
- It modestly increases the share of spending on extremely cost-effective causes like AI existential risk and some animal welfare interventions, and
- Some new “hits” are found over time.

The **high end** (\$100–\$500 per QALY) reflects scenarios where:

- Only a fraction of research is taken up by major funders,
- Most cause comparisons confirm the status quo rather than changing it, and
- GPR mainly tweaks at the margins of already good portfolios.

Even at the high end, GPR is **competitive with many object-level cause areas** (for example, climate, political advocacy, or standard health and education projects), because it acts as a **multiplier** on large funding flows rather than as a direct implementation charity.

---

## Start time and duration

Because GPR mostly affects **how large actors allocate money over time**, its impact is:

- **Delayed** (research must be done, evaluated, and translated into strategy), but
- **Persistent** (once a foundation or government changes cause priorities, those changes can last for years or decades).

### Start time

- **Point estimate:** benefits begin about **7 years** after the donation.

**Reasoning (brief):**

- A typical pattern is:
  - 1–3 years to conduct and publish research (for example at GPI or a foundation’s cause-prioritisation team).
  - 1–3 years for major funders to absorb the results, update their frameworks, and make new strategic commitments or grant programmes based on the research.
- Additional lag arises because grants often fund multi-year projects that take time to ramp up.

A 7-year start time captures this full cycle from **funding GPR → published work → strategic updates → money reaching downstream charities**.

### Duration

- **Point estimate:** impact lasts about **20 years**.

**Reasoning (brief):**

- Foundations like Open Philanthropy make **multi-decade commitments** to cause areas, guided by their high-level cause-prioritisation frameworks ([Open Phil notable lessons](https://www.openphilanthropy.org/notable-lessons/)).
- Once a funder decides that, say, biosecurity or AI existential risk deserves a major, sustained programme, that judgement can persist for **decades**, even as specific grants change.
- Empirically, the “big picture” shifts in EA cause prioritisation (for example, adding AI safety, pandemics, or animal welfare as major causes) have had enduring influence over at least 10–15 years so far, and may continue for much longer.

For modelling, we assume that the **extra QALYs from a marginal GPR donation are spread roughly evenly over a 20-year window**, starting about 7 years after the donation.

---

## Summary

Global priorities research sits “upstream” of many other cause areas. It does not vaccinate children, prevent nuclear war, or feed factory-farmed animals directly. Instead, it tries to answer:

> “Given our limited resources, which problems, interventions, and strategies will do the most good?”

Past work of this kind has:

- Helped establish **global health & development**, **animal welfare**, **AI existential risk**, and **pandemic prevention** as top priorities,
- Influenced **billions of dollars** in philanthropic commitments, and
- Shaped the very idea that some causes can be **orders of magnitude more effective** than others.

Under conservative assumptions that:

- Focus only on human QALYs over the next few decades,
- Exclude community-building and public-facing theory (counted under Meta and Theory), and
- Allow for a large fraction of research to have little direct impact,

we estimate that **top Global Priorities Research charities achieve around \$10 per QALY**, with a wide uncertainty range of **\$1–\$500 per QALY**, and that the resulting QALYs typically begin around **7 years** after a donation and continue for about **20 years**.

_Our current cost per life estimates are very approximate and we are looking for help improving them. Read about how you can contribute [here](https://github.com/impactlist/impactlist/blob/master/CONTRIBUTING.md)._

# Internal Notes
